{"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI40Az69V_dY","executionInfo":{"status":"ok","timestamp":1687180936732,"user_tz":-120,"elapsed":4901,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}},"outputId":"ba7cef96-08c2-43fd-fc51-fdb6a55b7168"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T14:29:51.728157Z","iopub.status.busy":"2023-05-23T14:29:51.727690Z","iopub.status.idle":"2023-05-23T14:29:56.821903Z","shell.execute_reply":"2023-05-23T14:29:56.820840Z","shell.execute_reply.started":"2023-05-23T14:29:51.728117Z"},"id":"gunQT5-9j4bh","trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","import os\n","import re\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XoRP78IZV18-","executionInfo":{"status":"ok","timestamp":1687179149427,"user_tz":-120,"elapsed":24239,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}},"outputId":"9b461289-8a63-4b10-c283-441862561859"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T14:29:56.824512Z","iopub.status.busy":"2023-05-23T14:29:56.823881Z","iopub.status.idle":"2023-05-23T14:30:57.402827Z","shell.execute_reply":"2023-05-23T14:30:57.401796Z","shell.execute_reply.started":"2023-05-23T14:29:56.824477Z"},"trusted":true,"id":"j7oJhMFoVy7q"},"outputs":[],"source":["# df = pd.read_csv('bib_data_union_v3.csv')\n","df = pd.read_csv('/content/drive/MyDrive/bib_data_union_v3.csv.zip',compression='zip')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T14:30:57.409930Z","iopub.status.busy":"2023-05-23T14:30:57.407692Z","iopub.status.idle":"2023-05-23T14:30:57.416040Z","shell.execute_reply":"2023-05-23T14:30:57.415272Z","shell.execute_reply.started":"2023-05-23T14:30:57.409895Z"},"trusted":true,"id":"PgiFRYWvVy7r"},"outputs":[],"source":["model_name = \"distilbert-base-uncased\"\n","# model_name = \"prajjwal1/bert-mini\"\n","# model_name = \"microsoft/deberta-base\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T14:31:05.189720Z","iopub.status.busy":"2023-05-23T14:31:05.189353Z","iopub.status.idle":"2023-05-23T14:31:07.683426Z","shell.execute_reply":"2023-05-23T14:31:07.682349Z","shell.execute_reply.started":"2023-05-23T14:31:05.189691Z"},"id":"1ub_GthTj4bj","trusted":true},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","def encode(df):\n","          columnsToEncode = list(df[['style_name']])\n","          le = LabelEncoder()\n","          for feature in columnsToEncode:\n","              try:\n","                  df[feature] = le.fit_transform(df[feature])\n","              except:\n","                  print('Error encoding '+feature)\n","          return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T14:44:04.703356Z","iopub.status.busy":"2023-05-23T14:44:04.702963Z","iopub.status.idle":"2023-05-23T14:44:06.829129Z","shell.execute_reply":"2023-05-23T14:44:06.828173Z","shell.execute_reply.started":"2023-05-23T14:44:04.703324Z"},"trusted":true,"id":"8nNq2kyRVy7u"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","data = encode(df)\n","\n","train_data, val_data = train_test_split(data, test_size=0.2, random_state=2020)\n","train_data = train_data.reset_index()\n","val_data = val_data.reset_index()\n","\n","eval_data, val_data = train_test_split(data, test_size=0.017, random_state=0)\n","eval_data = eval_data.reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T14:32:01.256967Z","iopub.status.busy":"2023-05-23T14:32:01.256054Z","iopub.status.idle":"2023-05-23T14:32:06.525968Z","shell.execute_reply":"2023-05-23T14:32:06.525014Z","shell.execute_reply.started":"2023-05-23T14:32:01.256922Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"1G3j8HmuVy7x","executionInfo":{"status":"ok","timestamp":1687190358728,"user_tz":-120,"elapsed":4,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}},"outputId":"f1df00dc-8b64-4b53-9575-16d4c78eb370"},"outputs":[{"output_type":"stream","name":"stdout","text":["30522\n","30528\n"]}],"source":["new_tokens = [\"upword\", \"capword\", \"othword\", \"caplet\", \"smallet\", \"year\", \"num\", \"sp\"]\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","print(len(tokenizer))\n","tokenizer.add_tokens(new_tokens)\n","print(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T14:44:27.783132Z","iopub.status.busy":"2023-05-23T14:44:27.782749Z","iopub.status.idle":"2023-05-23T14:44:27.794885Z","shell.execute_reply":"2023-05-23T14:44:27.793792Z","shell.execute_reply.started":"2023-05-23T14:44:27.783100Z"},"trusted":true,"id":"57aAOQMQVy7y"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class MyDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","\n","        encoded_sent = tokenizer.encode_plus(\n","            text=self.data[\"tokenized_record\"][idx],  # Preprocess sentence\n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=150,                  # Max length to truncate/pad\n","#             pad_to_max_length=True,         # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True,      # Return attention mask\n","            truncation=True,\n","            padding=\"max_length\"\n","            )\n","\n","\n","        return {\"input_ids\": torch.tensor(encoded_sent.get('input_ids')), \"attention_mask\": torch.tensor(encoded_sent.get('attention_mask')), \"labels\": torch.tensor(self.data[\"style_name\"][idx])}\n","\n","    def create_dataloader(self, batch_size=40, num_workers=1, shuffle=False):\n","        return DataLoader(\n","            self,\n","            batch_size=batch_size,\n","            num_workers=num_workers,\n","            shuffle=shuffle\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T15:09:10.605564Z","iopub.status.busy":"2023-05-23T15:09:10.605190Z","iopub.status.idle":"2023-05-23T15:09:10.613219Z","shell.execute_reply":"2023-05-23T15:09:10.612118Z","shell.execute_reply.started":"2023-05-23T15:09:10.605535Z"},"trusted":true,"id":"3VMA5fiVVy7z"},"outputs":[],"source":["train_dataloader = MyDataset(train_data).create_dataloader(batch_size=600, shuffle=True, num_workers=1)\n","val_dataloader = MyDataset(val_data).create_dataloader(batch_size=600, num_workers=1)\n"]},{"cell_type":"code","source":["val_dataloader2 = MyDataset(val_data[:20000]).create_dataloader(batch_size=600, num_workers=1)"],"metadata":{"id":"CGYTWUOAVp7n"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T14:44:47.033666Z","iopub.status.busy":"2023-05-23T14:44:47.033035Z","iopub.status.idle":"2023-05-23T14:44:54.674641Z","shell.execute_reply":"2023-05-23T14:44:54.673655Z","shell.execute_reply.started":"2023-05-23T14:44:47.033630Z"},"id":"e_XR0A5Rj4bl","trusted":true},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","from transformers import PreTrainedModel\n","import transformers\n","\n","# Create the BertClassfier class\n","class DistilBertClassifier(PreTrainedModel):\n","    \"\"\"Bert Model for Classification Tasks.\n","    \"\"\"\n","    def __init__(self, bert_config, freeze_bert=False):\n","        \"\"\"\n","        @param    bert: a BertModel object\n","        @param    classifier: a torch.nn.Module classifier\n","        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n","        \"\"\"\n","        super().__init__(bert_config)\n","        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n","        D_in, H, D_out = 256, 50, 91\n","        # 64x128\n","\n","        # Instantiate BERT model\n","        self.bert = AutoModel.from_pretrained(model_name)\n","        # self.bert = transformers.BertModel(bert_config)\n","        self.bert.resize_token_embeddings(len(tokenizer))\n","\n","\n","        # self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n","        # self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n","\n","        # self.bert =\n","\n","        # Instantiate an one-layer feed-forward classifier\n","        self.classifier = nn.Sequential(\n","#             nn.Tanh(),\n","            nn.GELU(),\n","            nn.Linear(self.bert.config.hidden_size, 300),\n","            nn.GELU(),\n","            nn.Dropout(0.05),\n","            nn.Linear(300, 91)\n","        )\n","\n","#         Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","        Feed input to BERT and the classifier to compute logits.\n","        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n","                      max_length)\n","        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n","                      information with shape (batch_size, max_length)\n","        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n","                      num_labels)\n","        \"\"\"\n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids,\n","                            attention_mask=attention_mask)\n","#         outputs = self.bert(input_ids=input_ids)\n","\n","\n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","\n","        # Feed input to classifier to compute logits\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-05-23T14:44:54.676760Z","iopub.status.busy":"2023-05-23T14:44:54.676431Z","iopub.status.idle":"2023-05-23T14:44:54.758915Z","shell.execute_reply":"2023-05-23T14:44:54.757858Z","shell.execute_reply.started":"2023-05-23T14:44:54.676732Z"},"id":"6Vf1QPUGj4bl","outputId":"fe42023f-6401-4a25-ae89-3c6276cc5b61","trusted":true,"executionInfo":{"status":"ok","timestamp":1687190376891,"user_tz":-120,"elapsed":3,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","Device name: NVIDIA A100-SXM4-40GB\n"]}],"source":["import torch\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T14:49:22.946946Z","iopub.status.busy":"2023-05-23T14:49:22.946517Z","iopub.status.idle":"2023-05-23T14:49:22.957981Z","shell.execute_reply":"2023-05-23T14:49:22.956961Z","shell.execute_reply.started":"2023-05-23T14:49:22.946913Z"},"id":"UwTGWUAXj4bl","trusted":true},"outputs":[],"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import AutoConfig\n","\n","def initialize_model(epochs=4):\n","    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\"\"\"\n","    # Instantiate Bert Classifier\n","    bert_config = AutoConfig.from_pretrained(model_name)\n","    bert_config.update(\n","        {\"hidden_dropout_prob\": 0.1, \"layer_norm_eps\": 1e-7, \"max_position_embeddings\": 270, \"hidden_size\": 800, \"intermediate_size\": 500, \"num_hidden_layers\": 3,\n","         \"initializer_range\": 2}\n","        # {\"hidden_dropout_prob\": 0.05, \"layer_norm_eps\": 1e-7}\n","\n","    )\n","\n","    bert_classifier = DistilBertClassifier(bert_config, freeze_bert=False)\n","\n","    # Tell PyTorch to run the model on GPU\n","    bert_classifier.to(device)\n","\n","    # Create the optimizer\n","    lr = 2e-5\n","    optimizer = AdamW(\n","        bert_classifier.parameters(),\n","        lr=lr,  # Default learning rate\n","        eps=1e-8,  # Default epsilon value\n","    )\n","\n","    # Total number of training steps\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Set up the learning rate scheduler\n","\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","        optimizer,\n","        pct_start=0.0,\n","        max_lr=lr,\n","        steps_per_epoch=len(train_dataloader),\n","        epochs=epochs,\n","        final_div_factor=1e4,\n","    )\n","\n","    # scheduler = get_linear_schedule_with_warmup(optimizer,\n","    #                                             num_warmup_steps=0, # Default value\n","    #                                             num_training_steps=total_steps)\n","    return bert_classifier, optimizer, scheduler\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T15:08:30.838673Z","iopub.status.busy":"2023-05-23T15:08:30.838190Z","iopub.status.idle":"2023-05-23T15:08:30.861803Z","shell.execute_reply":"2023-05-23T15:08:30.860842Z","shell.execute_reply.started":"2023-05-23T15:08:30.838631Z"},"id":"Lt88P1Tdj4bm","trusted":true},"outputs":[],"source":["import random\n","import time\n","from sklearn.metrics import f1_score\n","\n","# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def set_seed(seed_value=32):\n","    \"\"\"Set seed for reproducibility.\n","    \"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False,accum_steps=3):\n","    \"\"\"Train the BertClassifier model.\n","    \"\"\"\n","    # Start training loop\n","    print(\"Start training...\\n\")\n","    for epoch_i in range(epochs):\n","        # =======================================\n","        #               Training\n","        # =======================================\n","        # Print the header of the result table\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","\n","        # Measure the elapsed time of each epoch\n","        t0_epoch, t0_batch = time.time(), time.time()\n","\n","        # Reset tracking variables at the beginning of each epoch\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","\n","        # Put the model into the training mode\n","        model.train()\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","\n","            batch_counts +=1\n","            # Load batch to GPU\n","            b_input_ids, b_attn_mask, b_labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n","\n","            # Zero out any previously calculated gradients\n","            #model.zero_grad()\n","\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            loss = loss / accum_steps\n","\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","\n","            # Update parameters and the learning rate\n","            if (step + 1) % accum_steps == 0:\n","              torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","              optimizer.step()\n","              scheduler.step()\n","              # Zero out any previously calculated gradients\n","              model.zero_grad()\n","\n","            # Print the loss values and time elapsed for every 20 batches\n","            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","                # Calculate time elapsed for 20 batches\n","                time_elapsed = time.time() - t0_batch\n","\n","                # Print training results\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                # Reset batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","            if (step % 500 == 0 and step != 0):\n","                val_loss, val_accuracy = evaluate(model, val_dataloader2)\n","\n","                # Print performance over the entire training data\n","                time_elapsed = time.time() - t0_epoch\n","\n","                print(f\"{epoch_i + 1:^7} | {'-':^7} | ------------ | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","                print(\"-\"*70)\n","                model.train()\n","                torch.save(model.state_dict(), f\"drive/MyDrive/bibtex_checkpoints/{step // 500}.pt\")\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*70)\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if evaluation == True:\n","            # After the completion of each training epoch, measure the model's performance\n","            # on our validation set.\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","\n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*70)\n","            torch.save(model.state_dict(), f\"drive/MyDrive/bibtex_checkpoints/eval_checkpoint.pt\")\n","        print(\"\\n\")\n","\n","    print(\"Training complete!\")\n","\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","\n","    truth = []\n","    predicted = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask, b_labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","        truth += list(b_labels.cpu())\n","        predicted += list(preds.cpu())\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    print(\"F1 macro:\", f1_score(truth, predicted, average=\"macro\"))\n","\n","    return val_loss, val_accuracy"]},{"cell_type":"code","source":["#SMALL DATASET - ATTEMPT 1\n","set_seed(69)    # Set seed for reproducibility\n","bert_classifier, optimizer, scheduler = initialize_model(epochs=10)\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=10, evaluation=True, accum_steps=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"csZ1PFz2wKWe","executionInfo":{"status":"ok","timestamp":1687167749559,"user_tz":-120,"elapsed":2336158,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}},"outputId":"4faf99ae-370e-4f11-cba2-1cb410bef362"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Start training...\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["   1    |   20    |   4.488133   |     -      |     -     |   34.24  \n","   1    |   40    |   4.387455   |     -      |     -     |   31.11  \n","   1    |   60    |   4.228094   |     -      |     -     |   31.12  \n","   1    |   80    |   4.060920   |     -      |     -     |   31.12  \n","   1    |   100   |   3.909374   |     -      |     -     |   31.12  \n","   1    |   120   |   3.759154   |     -      |     -     |   31.11  \n","   1    |   136   |   3.613258   |     -      |     -     |   23.64  \n","----------------------------------------------------------------------\n","F1 macro: 0.26044481575597794\n","   1    |    -    |   4.080021   |  3.492145  |   33.07   |  233.40  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   2    |   20    |   3.482789   |     -      |     -     |   33.15  \n","   2    |   40    |   3.331238   |     -      |     -     |   31.13  \n","   2    |   60    |   3.191976   |     -      |     -     |   31.14  \n","   2    |   80    |   3.055010   |     -      |     -     |   31.12  \n","   2    |   100   |   2.925800   |     -      |     -     |   31.13  \n","   2    |   120   |   2.787266   |     -      |     -     |   31.15  \n","   2    |   136   |   2.679664   |     -      |     -     |   23.65  \n","----------------------------------------------------------------------\n","F1 macro: 0.43739820687917275\n","   2    |    -    |   3.079117   |  2.513648  |   52.22   |  232.07  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   3    |   20    |   2.562190   |     -      |     -     |   33.22  \n","   3    |   40    |   2.443832   |     -      |     -     |   31.20  \n","   3    |   60    |   2.331079   |     -      |     -     |   31.19  \n","   3    |   80    |   2.232371   |     -      |     -     |   31.19  \n","   3    |   100   |   2.111553   |     -      |     -     |   31.21  \n","   3    |   120   |   2.021167   |     -      |     -     |   31.21  \n","   3    |   136   |   1.926676   |     -      |     -     |   23.70  \n","----------------------------------------------------------------------\n","F1 macro: 0.5393523573056657\n","   3    |    -    |   2.244035   |  1.777609  |   61.50   |  232.75  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   4    |   20    |   1.839920   |     -      |     -     |   33.21  \n","   4    |   40    |   1.767331   |     -      |     -     |   31.21  \n","   4    |   60    |   1.687385   |     -      |     -     |   31.21  \n","   4    |   80    |   1.611668   |     -      |     -     |   31.21  \n","   4    |   100   |   1.576950   |     -      |     -     |   31.21  \n","   4    |   120   |   1.509265   |     -      |     -     |   31.21  \n","   4    |   136   |   1.432462   |     -      |     -     |   23.70  \n","----------------------------------------------------------------------\n","F1 macro: 0.589333391876617\n","   4    |    -    |   1.639487   |  1.327847  |   65.37   |  232.48  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   5    |   20    |   1.401350   |     -      |     -     |   33.22  \n","   5    |   40    |   1.365393   |     -      |     -     |   31.25  \n","   5    |   60    |   1.312198   |     -      |     -     |   31.20  \n","   5    |   80    |   1.285393   |     -      |     -     |   31.20  \n","   5    |   100   |   1.230996   |     -      |     -     |   31.21  \n","   5    |   120   |   1.208819   |     -      |     -     |   31.22  \n","   5    |   136   |   1.185252   |     -      |     -     |   23.71  \n","----------------------------------------------------------------------\n","F1 macro: 0.6249853168242181\n","   5    |    -    |   1.287944   |  1.082734  |   67.47   |  232.53  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   6    |   20    |   1.157344   |     -      |     -     |   33.18  \n","   6    |   40    |   1.126097   |     -      |     -     |   31.19  \n","   6    |   60    |   1.112622   |     -      |     -     |   31.19  \n","   6    |   80    |   1.079544   |     -      |     -     |   31.20  \n","   6    |   100   |   1.077925   |     -      |     -     |   31.21  \n","   6    |   120   |   1.051324   |     -      |     -     |   31.19  \n","   6    |   136   |   1.051788   |     -      |     -     |   23.69  \n","----------------------------------------------------------------------\n","F1 macro: 0.6329116317465926\n","   6    |    -    |   1.095497   |  0.964165  |   68.77   |  232.71  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   7    |   20    |   1.021850   |     -      |     -     |   33.16  \n","   7    |   40    |   1.019626   |     -      |     -     |   31.13  \n","   7    |   60    |   1.006823   |     -      |     -     |   31.13  \n","   7    |   80    |   0.997645   |     -      |     -     |   31.12  \n","   7    |   100   |   0.978862   |     -      |     -     |   31.13  \n","   7    |   120   |   0.971699   |     -      |     -     |   31.14  \n","   7    |   136   |   0.961216   |     -      |     -     |   23.64  \n","----------------------------------------------------------------------\n","F1 macro: 0.6386443307761779\n","   7    |    -    |   0.995120   |  0.898273  |   69.23   |  231.96  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   8    |   20    |   0.965146   |     -      |     -     |   33.19  \n","   8    |   40    |   0.961957   |     -      |     -     |   31.19  \n","   8    |   60    |   0.938402   |     -      |     -     |   31.19  \n","   8    |   80    |   0.944570   |     -      |     -     |   31.20  \n","   8    |   100   |   0.921344   |     -      |     -     |   31.20  \n","   8    |   120   |   0.935426   |     -      |     -     |   31.22  \n","   8    |   136   |   0.918218   |     -      |     -     |   23.70  \n","----------------------------------------------------------------------\n","F1 macro: 0.645547813720093\n","   8    |    -    |   0.941559   |  0.868780  |   69.87   |  232.84  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   9    |   20    |   0.921875   |     -      |     -     |   33.14  \n","   9    |   40    |   0.934022   |     -      |     -     |   31.18  \n","   9    |   60    |   0.916403   |     -      |     -     |   31.18  \n","   9    |   80    |   0.912123   |     -      |     -     |   31.20  \n","   9    |   100   |   0.920972   |     -      |     -     |   31.19  \n","   9    |   120   |   0.911386   |     -      |     -     |   31.19  \n","   9    |   136   |   0.903147   |     -      |     -     |   23.71  \n","----------------------------------------------------------------------\n","F1 macro: 0.6463784920749823\n","   9    |    -    |   0.917575   |  0.857633  |   69.85   |  232.28  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  10    |   20    |   0.905314   |     -      |     -     |   33.22  \n","  10    |   40    |   0.918118   |     -      |     -     |   31.19  \n","  10    |   60    |   0.903666   |     -      |     -     |   31.20  \n","  10    |   80    |   0.912332   |     -      |     -     |   31.21  \n","  10    |   100   |   0.918613   |     -      |     -     |   31.21  \n","  10    |   120   |   0.914402   |     -      |     -     |   31.19  \n","  10    |   136   |   0.910368   |     -      |     -     |   23.69  \n","----------------------------------------------------------------------\n","F1 macro: 0.6476560713479451\n","  10    |    -    |   0.911826   |  0.855971  |   69.89   |  232.76  \n","----------------------------------------------------------------------\n","\n","\n","Training complete!\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T15:09:51.414081Z","iopub.status.busy":"2023-05-23T15:09:51.413712Z"},"trusted":true,"id":"yz2ymTnKVy74","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ae2451c-e8cd-42bd-cb25-b05fc96e03f8"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Start training...\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["   1    |   20    |   4.496846   |     -      |     -     |   34.28  \n","   1    |   40    |   4.399951   |     -      |     -     |   31.10  \n","   1    |   60    |   4.227060   |     -      |     -     |   31.10  \n","   1    |   80    |   4.056752   |     -      |     -     |   31.13  \n","   1    |   100   |   3.892649   |     -      |     -     |   31.13  \n","   1    |   120   |   3.719579   |     -      |     -     |   31.12  \n","   1    |   136   |   3.588303   |     -      |     -     |   23.63  \n","----------------------------------------------------------------------\n","F1 macro: 0.3269372811127633\n","   1    |    -    |   4.071288   |  3.443251  |   40.70   |  233.16  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   2    |   20    |   3.437327   |     -      |     -     |   33.13  \n","   2    |   40    |   3.272891   |     -      |     -     |   31.11  \n","   2    |   60    |   3.111338   |     -      |     -     |   31.11  \n","   2    |   80    |   2.951363   |     -      |     -     |   31.12  \n","   2    |   100   |   2.793973   |     -      |     -     |   31.11  \n","   2    |   120   |   2.646290   |     -      |     -     |   31.12  \n","   2    |   136   |   2.518222   |     -      |     -     |   23.63  \n","----------------------------------------------------------------------\n","F1 macro: 0.5014277740495778\n","   2    |    -    |   2.978048   |  2.331489  |   58.13   |  231.74  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   3    |   20    |   2.385822   |     -      |     -     |   33.16  \n","   3    |   40    |   2.238762   |     -      |     -     |   31.12  \n","   3    |   60    |   2.123313   |     -      |     -     |   31.11  \n","   3    |   80    |   2.016246   |     -      |     -     |   31.12  \n","   3    |   100   |   1.916986   |     -      |     -     |   31.12  \n","   3    |   120   |   1.807890   |     -      |     -     |   31.11  \n","   3    |   136   |   1.738827   |     -      |     -     |   23.63  \n","----------------------------------------------------------------------\n","F1 macro: 0.5760914676362131\n","   3    |    -    |   2.043704   |  1.571924  |   64.72   |  232.00  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   4    |   20    |   1.639589   |     -      |     -     |   33.11  \n","   4    |   40    |   1.574872   |     -      |     -     |   31.12  \n","   4    |   60    |   1.484559   |     -      |     -     |   31.11  \n","   4    |   80    |   1.419418   |     -      |     -     |   31.12  \n","   4    |   100   |   1.352713   |     -      |     -     |   31.11  \n","   4    |   120   |   1.286886   |     -      |     -     |   31.11  \n","   4    |   136   |   1.253150   |     -      |     -     |   23.63  \n","----------------------------------------------------------------------\n","F1 macro: 0.6112448405615228\n","   4    |    -    |   1.436867   |  1.126323  |   67.35   |  231.72  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   5    |   20    |   1.210110   |     -      |     -     |   33.13  \n","   5    |   40    |   1.142428   |     -      |     -     |   31.11  \n","   5    |   60    |   1.116033   |     -      |     -     |   31.11  \n","   5    |   80    |   1.079452   |     -      |     -     |   31.12  \n","   5    |   100   |   1.037271   |     -      |     -     |   31.13  \n","   5    |   120   |   1.003162   |     -      |     -     |   31.11  \n","   5    |   136   |   0.976212   |     -      |     -     |   23.62  \n","----------------------------------------------------------------------\n","F1 macro: 0.6348614789017021\n","   5    |    -    |   1.084662   |  0.902434  |   69.19   |  231.99  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   6    |   20    |   0.964115   |     -      |     -     |   33.27  \n","   6    |   40    |   0.927224   |     -      |     -     |   31.12  \n","   6    |   60    |   0.915779   |     -      |     -     |   31.12  \n","   6    |   80    |   0.897965   |     -      |     -     |   31.12  \n","   6    |   100   |   0.892187   |     -      |     -     |   31.11  \n","   6    |   120   |   0.856914   |     -      |     -     |   31.12  \n","   6    |   136   |   0.843105   |     -      |     -     |   23.62  \n","----------------------------------------------------------------------\n","F1 macro: 0.6359836405135066\n","   6    |    -    |   0.901734   |  0.792840  |   70.55   |  231.92  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   7    |   20    |   0.832603   |     -      |     -     |   33.12  \n","   7    |   40    |   0.824949   |     -      |     -     |   31.12  \n","   7    |   60    |   0.822712   |     -      |     -     |   31.13  \n","   7    |   80    |   0.784839   |     -      |     -     |   31.12  \n","   7    |   100   |   0.790688   |     -      |     -     |   31.11  \n","   7    |   120   |   0.765601   |     -      |     -     |   31.11  \n","   7    |   136   |   0.778219   |     -      |     -     |   23.63  \n","----------------------------------------------------------------------\n","F1 macro: 0.6538157088942899\n","   7    |    -    |   0.800817   |  0.724541  |   71.44   |  231.76  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   8    |   20    |   0.748555   |     -      |     -     |   33.15  \n","   8    |   40    |   0.737715   |     -      |     -     |   31.11  \n","   8    |   60    |   0.744047   |     -      |     -     |   31.12  \n","   8    |   80    |   0.736387   |     -      |     -     |   31.12  \n","   8    |   100   |   0.721386   |     -      |     -     |   31.12  \n","   8    |   120   |   0.726143   |     -      |     -     |   31.11  \n","   8    |   136   |   0.719710   |     -      |     -     |   23.63  \n","----------------------------------------------------------------------\n","F1 macro: 0.6717467557680401\n","   8    |    -    |   0.733931   |  0.681150  |   72.48   |  232.07  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   9    |   20    |   0.705870   |     -      |     -     |   33.12  \n","   9    |   40    |   0.707883   |     -      |     -     |   31.11  \n","   9    |   60    |   0.689620   |     -      |     -     |   31.12  \n","   9    |   80    |   0.677148   |     -      |     -     |   31.12  \n","   9    |   100   |   0.690331   |     -      |     -     |   31.11  \n","   9    |   120   |   0.675858   |     -      |     -     |   31.11  \n","   9    |   136   |   0.671516   |     -      |     -     |   23.62  \n","----------------------------------------------------------------------\n","F1 macro: 0.6780584899934242\n","   9    |    -    |   0.688937   |  0.653419  |   73.46   |  231.73  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  10    |   20    |   0.680510   |     -      |     -     |   33.13  \n"]}],"source":["# SMALL DATASET - ATTEMPT 2\n","set_seed(42)\n","bert_classifier, optimizer, scheduler = initialize_model(epochs=20)\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=20, evaluation=True, accum_steps=1)"]},{"cell_type":"code","source":["torch.save(bert_classifier.state_dict(), \"drive/MyDrive/checkpoint_last_20epoch.pt\")"],"metadata":{"id":"fpMXMkEIR_59"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zbZJN7kDVy74","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687144382752,"user_tz":-120,"elapsed":27822332,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}},"outputId":"82cd7ff7-8b7b-4f2c-b355-af6a3fb50b87"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Start training...\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["   1    |   20    |   4.485361   |     -      |     -     |   34.67  \n","   1    |   40    |   4.384121   |     -      |     -     |   31.13  \n","   1    |   60    |   4.241343   |     -      |     -     |   31.14  \n","   1    |   80    |   4.082496   |     -      |     -     |   31.15  \n","   1    |   100   |   3.935807   |     -      |     -     |   31.15  \n","   1    |   120   |   3.776402   |     -      |     -     |   31.15  \n","   1    |   140   |   3.624148   |     -      |     -     |   31.13  \n","   1    |   160   |   3.462833   |     -      |     -     |   31.13  \n","   1    |   180   |   3.306221   |     -      |     -     |   31.14  \n","   1    |   200   |   3.167774   |     -      |     -     |   31.17  \n","   1    |   220   |   3.021670   |     -      |     -     |   31.13  \n","   1    |   240   |   2.873523   |     -      |     -     |   31.14  \n","   1    |   260   |   2.723048   |     -      |     -     |   31.14  \n","   1    |   280   |   2.571031   |     -      |     -     |   31.14  \n","   1    |   300   |   2.442184   |     -      |     -     |   31.13  \n","   1    |   320   |   2.313625   |     -      |     -     |   31.13  \n","   1    |   340   |   2.164010   |     -      |     -     |   31.13  \n","   1    |   360   |   2.059445   |     -      |     -     |   31.13  \n","   1    |   380   |   1.928338   |     -      |     -     |   31.13  \n","   1    |   400   |   1.831797   |     -      |     -     |   31.14  \n","   1    |   420   |   1.719187   |     -      |     -     |   31.13  \n","   1    |   440   |   1.633206   |     -      |     -     |   31.14  \n","   1    |   460   |   1.552458   |     -      |     -     |   31.13  \n","   1    |   480   |   1.472718   |     -      |     -     |   31.13  \n","   1    |   500   |   1.388789   |     -      |     -     |   31.14  \n","F1 macro: 0.5978797389559998\n","   1    |    -    | ------------ |  1.238876  |   65.92   |  801.54  \n","----------------------------------------------------------------------\n","   1    |   520   |   1.311779   |     -      |     -     |   51.35  \n","   1    |   540   |   1.247149   |     -      |     -     |   31.14  \n","   1    |   560   |   1.195234   |     -      |     -     |   31.14  \n","   1    |   580   |   1.141996   |     -      |     -     |   31.15  \n","   1    |   600   |   1.100107   |     -      |     -     |   31.14  \n","   1    |   620   |   1.059181   |     -      |     -     |   31.14  \n","   1    |   640   |   1.023397   |     -      |     -     |   31.14  \n","   1    |   660   |   0.991563   |     -      |     -     |   31.13  \n","   1    |   680   |   0.955610   |     -      |     -     |   31.14  \n","   1    |   700   |   0.927214   |     -      |     -     |   31.14  \n","   1    |   720   |   0.911111   |     -      |     -     |   31.13  \n","   1    |   740   |   0.895987   |     -      |     -     |   31.14  \n","   1    |   760   |   0.864564   |     -      |     -     |   31.13  \n","   1    |   780   |   0.857267   |     -      |     -     |   31.14  \n","   1    |   800   |   0.827697   |     -      |     -     |   31.14  \n","   1    |   820   |   0.813039   |     -      |     -     |   31.13  \n","   1    |   840   |   0.798946   |     -      |     -     |   31.13  \n","   1    |   860   |   0.783420   |     -      |     -     |   31.14  \n","   1    |   880   |   0.778429   |     -      |     -     |   31.14  \n","   1    |   900   |   0.762733   |     -      |     -     |   31.13  \n","   1    |   920   |   0.754907   |     -      |     -     |   31.13  \n","   1    |   940   |   0.723811   |     -      |     -     |   31.13  \n","   1    |   960   |   0.727435   |     -      |     -     |   31.14  \n","   1    |   980   |   0.727830   |     -      |     -     |   31.14  \n","   1    |  1000   |   0.724683   |     -      |     -     |   31.14  \n","F1 macro: 0.6719108399899587\n","   1    |    -    | ------------ |  0.658606  |   72.90   |  1599.81 \n","----------------------------------------------------------------------\n","   1    |  1020   |   0.712498   |     -      |     -     |   50.99  \n","   1    |  1040   |   0.694907   |     -      |     -     |   31.14  \n","   1    |  1060   |   0.669397   |     -      |     -     |   31.14  \n","   1    |  1080   |   0.679512   |     -      |     -     |   31.13  \n","   1    |  1100   |   0.667414   |     -      |     -     |   31.14  \n","   1    |  1120   |   0.674174   |     -      |     -     |   31.14  \n","   1    |  1140   |   0.673962   |     -      |     -     |   31.14  \n","   1    |  1160   |   0.659865   |     -      |     -     |   31.14  \n","   1    |  1180   |   0.655524   |     -      |     -     |   31.13  \n","   1    |  1200   |   0.633842   |     -      |     -     |   31.14  \n","   1    |  1220   |   0.657452   |     -      |     -     |   31.13  \n","   1    |  1240   |   0.645669   |     -      |     -     |   31.13  \n","   1    |  1260   |   0.634748   |     -      |     -     |   31.14  \n","   1    |  1280   |   0.635804   |     -      |     -     |   31.14  \n","   1    |  1300   |   0.629851   |     -      |     -     |   31.14  \n","   1    |  1320   |   0.633282   |     -      |     -     |   31.14  \n","   1    |  1340   |   0.632058   |     -      |     -     |   31.13  \n","   1    |  1360   |   0.606863   |     -      |     -     |   31.13  \n","   1    |  1380   |   0.627283   |     -      |     -     |   31.13  \n","   1    |  1400   |   0.612631   |     -      |     -     |   31.14  \n","   1    |  1420   |   0.600107   |     -      |     -     |   31.13  \n","   1    |  1440   |   0.590277   |     -      |     -     |   31.13  \n","   1    |  1460   |   0.607012   |     -      |     -     |   31.14  \n","   1    |  1480   |   0.599180   |     -      |     -     |   31.14  \n","   1    |  1500   |   0.603012   |     -      |     -     |   31.13  \n","F1 macro: 0.6934878018583248\n","   1    |    -    | ------------ |  0.560544  |   74.38   |  2398.04 \n","----------------------------------------------------------------------\n","   1    |  1520   |   0.589945   |     -      |     -     |   50.93  \n","   1    |  1540   |   0.593937   |     -      |     -     |   31.14  \n","   1    |  1560   |   0.585158   |     -      |     -     |   31.13  \n","   1    |  1580   |   0.580299   |     -      |     -     |   31.13  \n","   1    |  1600   |   0.595188   |     -      |     -     |   31.13  \n","   1    |  1620   |   0.595517   |     -      |     -     |   31.13  \n","   1    |  1640   |   0.581850   |     -      |     -     |   31.13  \n","   1    |  1660   |   0.577580   |     -      |     -     |   31.14  \n","   1    |  1680   |   0.583805   |     -      |     -     |   31.13  \n","   1    |  1700   |   0.566593   |     -      |     -     |   31.14  \n","   1    |  1720   |   0.572983   |     -      |     -     |   31.13  \n","   1    |  1740   |   0.573757   |     -      |     -     |   31.14  \n","   1    |  1760   |   0.563640   |     -      |     -     |   31.14  \n","   1    |  1780   |   0.562784   |     -      |     -     |   31.14  \n","   1    |  1800   |   0.567144   |     -      |     -     |   31.13  \n","   1    |  1820   |   0.553848   |     -      |     -     |   31.13  \n","   1    |  1840   |   0.564370   |     -      |     -     |   31.13  \n","   1    |  1860   |   0.570579   |     -      |     -     |   31.14  \n","   1    |  1880   |   0.557768   |     -      |     -     |   31.14  \n","   1    |  1900   |   0.549330   |     -      |     -     |   31.13  \n","   1    |  1920   |   0.551305   |     -      |     -     |   31.14  \n","   1    |  1940   |   0.557151   |     -      |     -     |   31.13  \n","   1    |  1960   |   0.565455   |     -      |     -     |   31.12  \n","   1    |  1980   |   0.553418   |     -      |     -     |   31.12  \n","   1    |  2000   |   0.549999   |     -      |     -     |   31.13  \n","F1 macro: 0.6990716692888657\n","   1    |    -    | ------------ |  0.526988  |   74.57   |  3196.42 \n","----------------------------------------------------------------------\n","   1    |  2020   |   0.553053   |     -      |     -     |   51.13  \n","   1    |  2040   |   0.552568   |     -      |     -     |   31.13  \n","   1    |  2060   |   0.562191   |     -      |     -     |   31.14  \n","   1    |  2080   |   0.554181   |     -      |     -     |   31.14  \n","   1    |  2100   |   0.546925   |     -      |     -     |   31.13  \n","   1    |  2120   |   0.575618   |     -      |     -     |   31.13  \n","   1    |  2140   |   0.546901   |     -      |     -     |   31.13  \n","   1    |  2160   |   0.539031   |     -      |     -     |   31.13  \n","   1    |  2180   |   0.544848   |     -      |     -     |   31.13  \n","   1    |  2200   |   0.549124   |     -      |     -     |   31.13  \n","   1    |  2220   |   0.557629   |     -      |     -     |   31.13  \n","   1    |  2240   |   0.532550   |     -      |     -     |   31.13  \n","   1    |  2260   |   0.547682   |     -      |     -     |   31.13  \n","   1    |  2280   |   0.539823   |     -      |     -     |   31.14  \n","   1    |  2300   |   0.534485   |     -      |     -     |   31.13  \n","   1    |  2320   |   0.542491   |     -      |     -     |   31.13  \n","   1    |  2340   |   0.539934   |     -      |     -     |   31.13  \n","   1    |  2360   |   0.543010   |     -      |     -     |   31.13  \n","   1    |  2380   |   0.528144   |     -      |     -     |   31.14  \n","   1    |  2400   |   0.531163   |     -      |     -     |   31.13  \n","   1    |  2420   |   0.528046   |     -      |     -     |   31.13  \n","   1    |  2440   |   0.528592   |     -      |     -     |   31.13  \n","   1    |  2460   |   0.529963   |     -      |     -     |   31.13  \n","   1    |  2480   |   0.535045   |     -      |     -     |   31.14  \n","   1    |  2500   |   0.531224   |     -      |     -     |   31.13  \n","F1 macro: 0.6972073479477557\n","   1    |    -    | ------------ |  0.509731  |   74.83   |  3994.49 \n","----------------------------------------------------------------------\n","   1    |  2520   |   0.535447   |     -      |     -     |   50.88  \n","   1    |  2540   |   0.546368   |     -      |     -     |   31.13  \n","   1    |  2560   |   0.533888   |     -      |     -     |   31.13  \n","   1    |  2580   |   0.548826   |     -      |     -     |   31.13  \n","   1    |  2600   |   0.536122   |     -      |     -     |   31.14  \n","   1    |  2620   |   0.533148   |     -      |     -     |   31.13  \n","   1    |  2640   |   0.538867   |     -      |     -     |   31.13  \n","   1    |  2660   |   0.532327   |     -      |     -     |   31.14  \n","   1    |  2680   |   0.518045   |     -      |     -     |   31.13  \n","   1    |  2700   |   0.538186   |     -      |     -     |   31.13  \n","   1    |  2720   |   0.520798   |     -      |     -     |   31.13  \n","   1    |  2740   |   0.521727   |     -      |     -     |   31.13  \n","   1    |  2760   |   0.524604   |     -      |     -     |   31.13  \n","   1    |  2780   |   0.526812   |     -      |     -     |   31.13  \n","   1    |  2800   |   0.532660   |     -      |     -     |   31.13  \n","   1    |  2820   |   0.520420   |     -      |     -     |   31.13  \n","   1    |  2840   |   0.529112   |     -      |     -     |   31.13  \n","   1    |  2860   |   0.525452   |     -      |     -     |   31.14  \n","   1    |  2880   |   0.531737   |     -      |     -     |   31.14  \n","   1    |  2900   |   0.526671   |     -      |     -     |   31.13  \n","   1    |  2920   |   0.516673   |     -      |     -     |   31.14  \n","   1    |  2940   |   0.536578   |     -      |     -     |   31.13  \n","   1    |  2960   |   0.533842   |     -      |     -     |   31.13  \n","   1    |  2980   |   0.511881   |     -      |     -     |   31.13  \n","   1    |  3000   |   0.538876   |     -      |     -     |   31.13  \n","F1 macro: 0.7070331250341996\n","   1    |    -    | ------------ |  0.498777  |   75.16   |  4792.89 \n","----------------------------------------------------------------------\n","   1    |  3020   |   0.522474   |     -      |     -     |   51.24  \n","   1    |  3040   |   0.523906   |     -      |     -     |   31.14  \n","   1    |  3060   |   0.517651   |     -      |     -     |   31.14  \n","   1    |  3080   |   0.523222   |     -      |     -     |   31.13  \n","   1    |  3100   |   0.518572   |     -      |     -     |   31.13  \n","   1    |  3120   |   0.516555   |     -      |     -     |   31.13  \n","   1    |  3140   |   0.519375   |     -      |     -     |   31.13  \n","   1    |  3160   |   0.532702   |     -      |     -     |   31.13  \n","   1    |  3180   |   0.521048   |     -      |     -     |   31.13  \n","   1    |  3200   |   0.521823   |     -      |     -     |   31.13  \n","   1    |  3220   |   0.537408   |     -      |     -     |   31.13  \n","   1    |  3240   |   0.531504   |     -      |     -     |   31.13  \n","   1    |  3260   |   0.522419   |     -      |     -     |   31.13  \n","   1    |  3280   |   0.528558   |     -      |     -     |   31.14  \n","   1    |  3300   |   0.523954   |     -      |     -     |   31.13  \n","   1    |  3320   |   0.511422   |     -      |     -     |   31.13  \n","   1    |  3340   |   0.521565   |     -      |     -     |   31.13  \n","   1    |  3360   |   0.518063   |     -      |     -     |   31.13  \n","   1    |  3380   |   0.507821   |     -      |     -     |   31.12  \n","   1    |  3400   |   0.521735   |     -      |     -     |   31.13  \n","   1    |  3420   |   0.515273   |     -      |     -     |   31.13  \n","   1    |  3440   |   0.502664   |     -      |     -     |   31.13  \n","   1    |  3460   |   0.524829   |     -      |     -     |   31.13  \n","   1    |  3480   |   0.508654   |     -      |     -     |   31.13  \n","   1    |  3500   |   0.512650   |     -      |     -     |   31.13  \n","F1 macro: 0.7106948635998624\n","   1    |    -    | ------------ |  0.493629  |   74.95   |  5590.96 \n","----------------------------------------------------------------------\n","   1    |  3520   |   0.517975   |     -      |     -     |   50.90  \n","   1    |  3540   |   0.523753   |     -      |     -     |   31.14  \n","   1    |  3560   |   0.512369   |     -      |     -     |   31.13  \n","   1    |  3580   |   0.514921   |     -      |     -     |   31.13  \n","   1    |  3600   |   0.513105   |     -      |     -     |   31.14  \n","   1    |  3620   |   0.506535   |     -      |     -     |   31.13  \n","   1    |  3640   |   0.515743   |     -      |     -     |   31.13  \n","   1    |  3660   |   0.507929   |     -      |     -     |   31.13  \n","   1    |  3680   |   0.515136   |     -      |     -     |   31.13  \n","   1    |  3700   |   0.509601   |     -      |     -     |   31.14  \n","   1    |  3720   |   0.513934   |     -      |     -     |   31.14  \n","   1    |  3740   |   0.495783   |     -      |     -     |   31.13  \n","   1    |  3760   |   0.523880   |     -      |     -     |   31.13  \n","   1    |  3780   |   0.520527   |     -      |     -     |   31.14  \n","   1    |  3800   |   0.513345   |     -      |     -     |   31.14  \n","   1    |  3820   |   0.509329   |     -      |     -     |   31.13  \n","   1    |  3840   |   0.505130   |     -      |     -     |   31.14  \n","   1    |  3860   |   0.511131   |     -      |     -     |   31.13  \n","   1    |  3880   |   0.519634   |     -      |     -     |   31.14  \n","   1    |  3900   |   0.507485   |     -      |     -     |   31.13  \n","   1    |  3920   |   0.503934   |     -      |     -     |   31.13  \n","   1    |  3940   |   0.500917   |     -      |     -     |   31.13  \n","   1    |  3960   |   0.504907   |     -      |     -     |   31.13  \n","   1    |  3980   |   0.500844   |     -      |     -     |   31.13  \n","   1    |  4000   |   0.525918   |     -      |     -     |   31.13  \n","F1 macro: 0.7130574903085607\n","   1    |    -    | ------------ |  0.488328  |   75.50   |  6389.30 \n","----------------------------------------------------------------------\n","   1    |  4020   |   0.500290   |     -      |     -     |   51.13  \n","   1    |  4040   |   0.502049   |     -      |     -     |   31.14  \n","   1    |  4060   |   0.511989   |     -      |     -     |   31.13  \n","   1    |  4080   |   0.513191   |     -      |     -     |   31.13  \n","   1    |  4100   |   0.502292   |     -      |     -     |   31.13  \n","   1    |  4120   |   0.502823   |     -      |     -     |   31.14  \n","   1    |  4140   |   0.510219   |     -      |     -     |   31.13  \n","   1    |  4160   |   0.510866   |     -      |     -     |   31.14  \n","   1    |  4180   |   0.504062   |     -      |     -     |   31.13  \n","   1    |  4200   |   0.510044   |     -      |     -     |   31.13  \n","   1    |  4220   |   0.514672   |     -      |     -     |   31.14  \n","   1    |  4240   |   0.502702   |     -      |     -     |   31.13  \n","   1    |  4260   |   0.516572   |     -      |     -     |   31.13  \n","   1    |  4280   |   0.511532   |     -      |     -     |   31.13  \n","   1    |  4300   |   0.505383   |     -      |     -     |   31.14  \n","   1    |  4320   |   0.506654   |     -      |     -     |   31.13  \n","   1    |  4340   |   0.504232   |     -      |     -     |   31.13  \n","   1    |  4360   |   0.502523   |     -      |     -     |   31.14  \n","   1    |  4380   |   0.514131   |     -      |     -     |   31.14  \n","   1    |  4400   |   0.513894   |     -      |     -     |   31.13  \n","   1    |  4420   |   0.499926   |     -      |     -     |   31.13  \n","   1    |  4440   |   0.504221   |     -      |     -     |   31.14  \n","   1    |  4460   |   0.520238   |     -      |     -     |   31.14  \n","   1    |  4480   |   0.505606   |     -      |     -     |   31.13  \n","   1    |  4500   |   0.527264   |     -      |     -     |   31.13  \n","F1 macro: 0.713568157619198\n","   1    |    -    | ------------ |  0.485003  |   74.92   |  7187.42 \n","----------------------------------------------------------------------\n","   1    |  4520   |   0.507128   |     -      |     -     |   50.91  \n","   1    |  4540   |   0.509240   |     -      |     -     |   31.13  \n","   1    |  4560   |   0.509562   |     -      |     -     |   31.14  \n","   1    |  4580   |   0.508270   |     -      |     -     |   31.13  \n","   1    |  4600   |   0.501686   |     -      |     -     |   31.13  \n","   1    |  4620   |   0.512195   |     -      |     -     |   31.13  \n","   1    |  4640   |   0.497980   |     -      |     -     |   31.13  \n","   1    |  4660   |   0.498015   |     -      |     -     |   31.13  \n","   1    |  4680   |   0.513655   |     -      |     -     |   31.13  \n","   1    |  4700   |   0.511377   |     -      |     -     |   31.14  \n","   1    |  4720   |   0.501965   |     -      |     -     |   31.13  \n","   1    |  4740   |   0.492231   |     -      |     -     |   31.12  \n","   1    |  4760   |   0.501709   |     -      |     -     |   31.12  \n","   1    |  4780   |   0.501918   |     -      |     -     |   31.13  \n","   1    |  4800   |   0.521114   |     -      |     -     |   31.14  \n","   1    |  4820   |   0.511175   |     -      |     -     |   31.13  \n","   1    |  4840   |   0.498852   |     -      |     -     |   31.13  \n","   1    |  4860   |   0.505198   |     -      |     -     |   31.13  \n","   1    |  4880   |   0.500476   |     -      |     -     |   31.13  \n","   1    |  4900   |   0.497225   |     -      |     -     |   31.13  \n","   1    |  4920   |   0.494812   |     -      |     -     |   31.13  \n","   1    |  4940   |   0.508785   |     -      |     -     |   31.13  \n","   1    |  4960   |   0.490998   |     -      |     -     |   31.13  \n","   1    |  4980   |   0.504748   |     -      |     -     |   31.13  \n","   1    |  5000   |   0.493409   |     -      |     -     |   31.13  \n","F1 macro: 0.7264393672225227\n","   1    |    -    | ------------ |  0.480758  |   75.54   |  7985.47 \n","----------------------------------------------------------------------\n","   1    |  5020   |   0.501750   |     -      |     -     |   50.92  \n","   1    |  5040   |   0.502597   |     -      |     -     |   31.13  \n","   1    |  5060   |   0.492603   |     -      |     -     |   31.13  \n","   1    |  5080   |   0.502186   |     -      |     -     |   31.13  \n","   1    |  5100   |   0.492536   |     -      |     -     |   31.13  \n","   1    |  5120   |   0.508718   |     -      |     -     |   31.13  \n","   1    |  5140   |   0.508046   |     -      |     -     |   31.13  \n","   1    |  5160   |   0.503379   |     -      |     -     |   31.13  \n","   1    |  5180   |   0.491161   |     -      |     -     |   31.13  \n","   1    |  5200   |   0.509449   |     -      |     -     |   31.13  \n","   1    |  5220   |   0.495532   |     -      |     -     |   31.13  \n","   1    |  5240   |   0.512011   |     -      |     -     |   31.13  \n","   1    |  5260   |   0.502904   |     -      |     -     |   31.13  \n","   1    |  5280   |   0.497498   |     -      |     -     |   31.13  \n","   1    |  5300   |   0.501104   |     -      |     -     |   31.13  \n","   1    |  5320   |   0.503852   |     -      |     -     |   31.13  \n","   1    |  5340   |   0.494578   |     -      |     -     |   31.14  \n","   1    |  5360   |   0.502678   |     -      |     -     |   31.13  \n","   1    |  5380   |   0.505224   |     -      |     -     |   31.13  \n","   1    |  5400   |   0.493585   |     -      |     -     |   31.13  \n","   1    |  5420   |   0.504639   |     -      |     -     |   31.13  \n","   1    |  5440   |   0.506947   |     -      |     -     |   31.14  \n","   1    |  5460   |   0.510164   |     -      |     -     |   31.13  \n","   1    |  5480   |   0.490397   |     -      |     -     |   31.13  \n","   1    |  5500   |   0.507194   |     -      |     -     |   31.13  \n","F1 macro: 0.7172325015087739\n","   1    |    -    | ------------ |  0.479816  |   75.44   |  8783.79 \n","----------------------------------------------------------------------\n","   1    |  5520   |   0.497657   |     -      |     -     |   51.17  \n","   1    |  5540   |   0.487582   |     -      |     -     |   31.13  \n","   1    |  5560   |   0.503100   |     -      |     -     |   31.13  \n","   1    |  5580   |   0.498436   |     -      |     -     |   31.13  \n","   1    |  5600   |   0.492254   |     -      |     -     |   31.13  \n","   1    |  5620   |   0.486059   |     -      |     -     |   31.13  \n","   1    |  5640   |   0.495070   |     -      |     -     |   31.13  \n","   1    |  5660   |   0.489669   |     -      |     -     |   31.14  \n","   1    |  5680   |   0.485571   |     -      |     -     |   31.13  \n","   1    |  5700   |   0.501479   |     -      |     -     |   31.13  \n","   1    |  5720   |   0.484413   |     -      |     -     |   31.14  \n","   1    |  5740   |   0.505349   |     -      |     -     |   31.14  \n","   1    |  5760   |   0.488578   |     -      |     -     |   31.14  \n","   1    |  5780   |   0.490893   |     -      |     -     |   31.13  \n","   1    |  5800   |   0.485716   |     -      |     -     |   31.13  \n","   1    |  5820   |   0.495026   |     -      |     -     |   31.13  \n","   1    |  5840   |   0.488030   |     -      |     -     |   31.14  \n","   1    |  5860   |   0.491871   |     -      |     -     |   31.13  \n","   1    |  5880   |   0.501522   |     -      |     -     |   31.14  \n","   1    |  5900   |   0.498286   |     -      |     -     |   31.13  \n","   1    |  5920   |   0.497945   |     -      |     -     |   31.13  \n","   1    |  5940   |   0.488498   |     -      |     -     |   31.14  \n","   1    |  5960   |   0.503531   |     -      |     -     |   31.14  \n","   1    |  5980   |   0.492944   |     -      |     -     |   31.13  \n","   1    |  6000   |   0.501724   |     -      |     -     |   31.13  \n","F1 macro: 0.7319366197119724\n","   1    |    -    | ------------ |  0.477296  |   75.22   |  9581.96 \n","----------------------------------------------------------------------\n","   1    |  6020   |   0.490625   |     -      |     -     |   50.98  \n","   1    |  6040   |   0.486776   |     -      |     -     |   31.15  \n","   1    |  6060   |   0.495270   |     -      |     -     |   31.15  \n","   1    |  6080   |   0.494941   |     -      |     -     |   31.15  \n","   1    |  6100   |   0.489353   |     -      |     -     |   31.14  \n","   1    |  6120   |   0.499168   |     -      |     -     |   31.14  \n","   1    |  6140   |   0.489240   |     -      |     -     |   31.14  \n","   1    |  6160   |   0.498546   |     -      |     -     |   31.15  \n","   1    |  6180   |   0.496439   |     -      |     -     |   31.14  \n","   1    |  6200   |   0.500433   |     -      |     -     |   31.14  \n","   1    |  6220   |   0.497965   |     -      |     -     |   31.13  \n","   1    |  6240   |   0.488160   |     -      |     -     |   31.13  \n","   1    |  6260   |   0.501341   |     -      |     -     |   31.13  \n","   1    |  6280   |   0.503038   |     -      |     -     |   31.14  \n","   1    |  6300   |   0.487494   |     -      |     -     |   31.14  \n","   1    |  6320   |   0.494324   |     -      |     -     |   31.13  \n","   1    |  6340   |   0.494567   |     -      |     -     |   31.13  \n","   1    |  6360   |   0.494561   |     -      |     -     |   31.13  \n","   1    |  6380   |   0.493331   |     -      |     -     |   31.13  \n","   1    |  6400   |   0.494264   |     -      |     -     |   31.13  \n","   1    |  6420   |   0.480285   |     -      |     -     |   31.13  \n","   1    |  6440   |   0.489721   |     -      |     -     |   31.13  \n","   1    |  6460   |   0.488998   |     -      |     -     |   31.13  \n","   1    |  6480   |   0.501008   |     -      |     -     |   31.14  \n","   1    |  6500   |   0.495112   |     -      |     -     |   31.14  \n","F1 macro: 0.7192190237243493\n","   1    |    -    | ------------ |  0.476391  |   75.56   | 10380.53 \n","----------------------------------------------------------------------\n","   1    |  6520   |   0.485118   |     -      |     -     |   51.25  \n","   1    |  6540   |   0.491078   |     -      |     -     |   31.14  \n","   1    |  6560   |   0.490580   |     -      |     -     |   31.14  \n","   1    |  6580   |   0.493821   |     -      |     -     |   31.14  \n","   1    |  6600   |   0.480064   |     -      |     -     |   31.14  \n","   1    |  6620   |   0.494306   |     -      |     -     |   31.14  \n","   1    |  6640   |   0.497659   |     -      |     -     |   31.14  \n","   1    |  6660   |   0.489724   |     -      |     -     |   31.14  \n","   1    |  6680   |   0.495455   |     -      |     -     |   31.13  \n","   1    |  6700   |   0.495170   |     -      |     -     |   31.13  \n","   1    |  6720   |   0.497390   |     -      |     -     |   31.14  \n","   1    |  6740   |   0.493783   |     -      |     -     |   31.13  \n","   1    |  6760   |   0.495770   |     -      |     -     |   31.14  \n","   1    |  6780   |   0.491724   |     -      |     -     |   31.13  \n","   1    |  6800   |   0.508046   |     -      |     -     |   31.14  \n","   1    |  6820   |   0.497463   |     -      |     -     |   31.13  \n","   1    |  6840   |   0.500568   |     -      |     -     |   31.14  \n","   1    |  6860   |   0.477874   |     -      |     -     |   31.13  \n","   1    |  6880   |   0.490113   |     -      |     -     |   31.13  \n","   1    |  6900   |   0.493319   |     -      |     -     |   31.14  \n","   1    |  6920   |   0.496314   |     -      |     -     |   31.13  \n","   1    |  6940   |   0.488228   |     -      |     -     |   31.13  \n","   1    |  6960   |   0.489894   |     -      |     -     |   31.13  \n","   1    |  6980   |   0.495483   |     -      |     -     |   31.14  \n","   1    |  7000   |   0.515085   |     -      |     -     |   31.14  \n","F1 macro: 0.7205402757989838\n","   1    |    -    | ------------ |  0.474460  |   75.17   | 11178.74 \n","----------------------------------------------------------------------\n","   1    |  7020   |   0.491422   |     -      |     -     |   50.89  \n","   1    |  7040   |   0.489754   |     -      |     -     |   31.14  \n","   1    |  7060   |   0.492426   |     -      |     -     |   31.13  \n","   1    |  7080   |   0.479444   |     -      |     -     |   31.14  \n","   1    |  7100   |   0.483647   |     -      |     -     |   31.14  \n","   1    |  7120   |   0.485852   |     -      |     -     |   31.13  \n","   1    |  7140   |   0.484974   |     -      |     -     |   31.13  \n","   1    |  7160   |   0.498432   |     -      |     -     |   31.13  \n","   1    |  7180   |   0.486063   |     -      |     -     |   31.13  \n","   1    |  7200   |   0.490173   |     -      |     -     |   31.13  \n","   1    |  7220   |   0.484372   |     -      |     -     |   31.14  \n","   1    |  7240   |   0.489158   |     -      |     -     |   31.13  \n","   1    |  7260   |   0.493531   |     -      |     -     |   31.13  \n","   1    |  7280   |   0.492211   |     -      |     -     |   31.14  \n","   1    |  7300   |   0.488035   |     -      |     -     |   31.14  \n","   1    |  7320   |   0.498190   |     -      |     -     |   31.13  \n","   1    |  7340   |   0.480082   |     -      |     -     |   31.13  \n","   1    |  7360   |   0.490384   |     -      |     -     |   31.13  \n","   1    |  7380   |   0.489288   |     -      |     -     |   31.13  \n","   1    |  7400   |   0.483550   |     -      |     -     |   31.14  \n","   1    |  7420   |   0.490244   |     -      |     -     |   31.13  \n","   1    |  7440   |   0.488558   |     -      |     -     |   31.13  \n","   1    |  7460   |   0.482338   |     -      |     -     |   31.14  \n","   1    |  7480   |   0.496813   |     -      |     -     |   31.13  \n","   1    |  7500   |   0.486555   |     -      |     -     |   31.13  \n","F1 macro: 0.7192833760989457\n","   1    |    -    | ------------ |  0.471410  |   75.54   | 11977.10 \n","----------------------------------------------------------------------\n","   1    |  7520   |   0.491221   |     -      |     -     |   51.18  \n","   1    |  7540   |   0.492253   |     -      |     -     |   31.13  \n","   1    |  7560   |   0.482968   |     -      |     -     |   31.13  \n","   1    |  7580   |   0.489743   |     -      |     -     |   31.13  \n","   1    |  7600   |   0.486898   |     -      |     -     |   31.13  \n","   1    |  7620   |   0.496915   |     -      |     -     |   31.13  \n","   1    |  7640   |   0.479726   |     -      |     -     |   31.14  \n","   1    |  7660   |   0.497644   |     -      |     -     |   31.13  \n","   1    |  7680   |   0.483936   |     -      |     -     |   31.13  \n","   1    |  7700   |   0.476877   |     -      |     -     |   31.13  \n","   1    |  7720   |   0.480580   |     -      |     -     |   31.15  \n","   1    |  7740   |   0.500612   |     -      |     -     |   31.13  \n","   1    |  7760   |   0.489577   |     -      |     -     |   31.14  \n","   1    |  7780   |   0.508321   |     -      |     -     |   31.14  \n","   1    |  7800   |   0.486457   |     -      |     -     |   31.14  \n","   1    |  7820   |   0.482081   |     -      |     -     |   31.13  \n","   1    |  7840   |   0.473977   |     -      |     -     |   31.13  \n","   1    |  7860   |   0.493790   |     -      |     -     |   31.14  \n","   1    |  7880   |   0.477787   |     -      |     -     |   31.14  \n","   1    |  7900   |   0.483554   |     -      |     -     |   31.13  \n","   1    |  7920   |   0.491880   |     -      |     -     |   31.13  \n","   1    |  7940   |   0.487603   |     -      |     -     |   31.13  \n","   1    |  7960   |   0.487355   |     -      |     -     |   31.13  \n","   1    |  7980   |   0.495156   |     -      |     -     |   31.13  \n","   1    |  8000   |   0.488314   |     -      |     -     |   31.13  \n","F1 macro: 0.72024510098619\n","   1    |    -    | ------------ |  0.470295  |   75.75   | 12775.26 \n","----------------------------------------------------------------------\n","   1    |  8009   |   0.491834   |     -      |     -     |   34.21  \n","----------------------------------------------------------------------\n","F1 macro: 0.7132174339912417\n","   1    |    -    |   0.688533   |  0.477435  |   75.34   | 13909.03 \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   2    |   20    |   0.486702   |     -      |     -     |   33.69  \n","   2    |   40    |   0.490870   |     -      |     -     |   31.14  \n","   2    |   60    |   0.478660   |     -      |     -     |   31.15  \n","   2    |   80    |   0.481372   |     -      |     -     |   31.14  \n","   2    |   100   |   0.486868   |     -      |     -     |   31.14  \n","   2    |   120   |   0.482548   |     -      |     -     |   31.15  \n","   2    |   140   |   0.488774   |     -      |     -     |   31.14  \n","   2    |   160   |   0.488254   |     -      |     -     |   31.19  \n","   2    |   180   |   0.490493   |     -      |     -     |   31.14  \n","   2    |   200   |   0.487485   |     -      |     -     |   31.14  \n","   2    |   220   |   0.471581   |     -      |     -     |   31.14  \n","   2    |   240   |   0.482687   |     -      |     -     |   31.14  \n","   2    |   260   |   0.483642   |     -      |     -     |   31.13  \n","   2    |   280   |   0.477890   |     -      |     -     |   31.13  \n","   2    |   300   |   0.487873   |     -      |     -     |   31.13  \n","   2    |   320   |   0.486101   |     -      |     -     |   31.14  \n","   2    |   340   |   0.491767   |     -      |     -     |   31.13  \n","   2    |   360   |   0.493557   |     -      |     -     |   31.14  \n","   2    |   380   |   0.480096   |     -      |     -     |   31.13  \n","   2    |   400   |   0.489545   |     -      |     -     |   31.13  \n","   2    |   420   |   0.494980   |     -      |     -     |   31.13  \n","   2    |   440   |   0.481512   |     -      |     -     |   31.14  \n","   2    |   460   |   0.487017   |     -      |     -     |   31.14  \n","   2    |   480   |   0.489116   |     -      |     -     |   31.13  \n","   2    |   500   |   0.482407   |     -      |     -     |   31.14  \n","F1 macro: 0.719736291654782\n","   2    |    -    | ------------ |  0.469806  |   75.21   |  799.97  \n","----------------------------------------------------------------------\n","   2    |   520   |   0.480999   |     -      |     -     |   50.76  \n","   2    |   540   |   0.482169   |     -      |     -     |   31.14  \n","   2    |   560   |   0.493317   |     -      |     -     |   31.13  \n","   2    |   580   |   0.487945   |     -      |     -     |   31.14  \n","   2    |   600   |   0.477137   |     -      |     -     |   31.13  \n","   2    |   620   |   0.489027   |     -      |     -     |   31.13  \n","   2    |   640   |   0.481730   |     -      |     -     |   31.13  \n","   2    |   660   |   0.480127   |     -      |     -     |   31.14  \n","   2    |   680   |   0.489194   |     -      |     -     |   31.14  \n","   2    |   700   |   0.493282   |     -      |     -     |   31.14  \n","   2    |   720   |   0.484685   |     -      |     -     |   31.14  \n","   2    |   740   |   0.478541   |     -      |     -     |   31.13  \n","   2    |   760   |   0.481362   |     -      |     -     |   31.14  \n","   2    |   780   |   0.481653   |     -      |     -     |   31.14  \n","   2    |   800   |   0.481957   |     -      |     -     |   31.13  \n","   2    |   820   |   0.491210   |     -      |     -     |   31.13  \n","   2    |   840   |   0.475648   |     -      |     -     |   31.13  \n","   2    |   860   |   0.481703   |     -      |     -     |   31.13  \n","   2    |   880   |   0.478725   |     -      |     -     |   31.14  \n","   2    |   900   |   0.481101   |     -      |     -     |   31.13  \n","   2    |   920   |   0.477057   |     -      |     -     |   31.13  \n","   2    |   940   |   0.480369   |     -      |     -     |   31.14  \n","   2    |   960   |   0.478738   |     -      |     -     |   31.14  \n","   2    |   980   |   0.488114   |     -      |     -     |   31.13  \n","   2    |  1000   |   0.478181   |     -      |     -     |   31.13  \n","F1 macro: 0.7141044298678787\n","   2    |    -    | ------------ |  0.469171  |   75.62   |  1598.22 \n","----------------------------------------------------------------------\n","   2    |  1020   |   0.488700   |     -      |     -     |   50.99  \n","   2    |  1040   |   0.481337   |     -      |     -     |   31.13  \n","   2    |  1060   |   0.489360   |     -      |     -     |   31.14  \n","   2    |  1080   |   0.484200   |     -      |     -     |   31.14  \n","   2    |  1100   |   0.496204   |     -      |     -     |   31.14  \n","   2    |  1120   |   0.495618   |     -      |     -     |   31.14  \n","   2    |  1140   |   0.481155   |     -      |     -     |   31.13  \n","   2    |  1160   |   0.485494   |     -      |     -     |   31.14  \n","   2    |  1180   |   0.495042   |     -      |     -     |   31.14  \n","   2    |  1200   |   0.490935   |     -      |     -     |   31.13  \n","   2    |  1220   |   0.488605   |     -      |     -     |   31.13  \n","   2    |  1240   |   0.487289   |     -      |     -     |   31.13  \n","   2    |  1260   |   0.486437   |     -      |     -     |   31.13  \n","   2    |  1280   |   0.481000   |     -      |     -     |   31.13  \n","   2    |  1300   |   0.488628   |     -      |     -     |   31.14  \n","   2    |  1320   |   0.488957   |     -      |     -     |   31.14  \n","   2    |  1340   |   0.495564   |     -      |     -     |   31.14  \n","   2    |  1360   |   0.475381   |     -      |     -     |   31.13  \n","   2    |  1380   |   0.477663   |     -      |     -     |   31.13  \n","   2    |  1400   |   0.478605   |     -      |     -     |   31.14  \n","   2    |  1420   |   0.486821   |     -      |     -     |   31.13  \n","   2    |  1440   |   0.493699   |     -      |     -     |   31.13  \n","   2    |  1460   |   0.489344   |     -      |     -     |   31.14  \n","   2    |  1480   |   0.484881   |     -      |     -     |   31.14  \n","   2    |  1500   |   0.491197   |     -      |     -     |   31.13  \n","F1 macro: 0.7180733995087408\n","   2    |    -    | ------------ |  0.467755  |   75.69   |  2397.01 \n","----------------------------------------------------------------------\n","   2    |  1520   |   0.481555   |     -      |     -     |   51.53  \n","   2    |  1540   |   0.487504   |     -      |     -     |   31.13  \n","   2    |  1560   |   0.476715   |     -      |     -     |   31.13  \n","   2    |  1580   |   0.471967   |     -      |     -     |   31.14  \n","   2    |  1600   |   0.490389   |     -      |     -     |   31.14  \n","   2    |  1620   |   0.480912   |     -      |     -     |   31.13  \n","   2    |  1640   |   0.485204   |     -      |     -     |   31.14  \n","   2    |  1660   |   0.500165   |     -      |     -     |   31.13  \n","   2    |  1680   |   0.481785   |     -      |     -     |   31.13  \n","   2    |  1700   |   0.477112   |     -      |     -     |   31.14  \n","   2    |  1720   |   0.487257   |     -      |     -     |   31.13  \n","   2    |  1740   |   0.478425   |     -      |     -     |   31.13  \n","   2    |  1760   |   0.483710   |     -      |     -     |   31.13  \n","   2    |  1780   |   0.480656   |     -      |     -     |   31.14  \n","   2    |  1800   |   0.481802   |     -      |     -     |   31.13  \n","   2    |  1820   |   0.472533   |     -      |     -     |   31.14  \n","   2    |  1840   |   0.494572   |     -      |     -     |   31.14  \n","   2    |  1860   |   0.490608   |     -      |     -     |   31.14  \n","   2    |  1880   |   0.473982   |     -      |     -     |   31.13  \n","   2    |  1900   |   0.482919   |     -      |     -     |   31.13  \n","   2    |  1920   |   0.481626   |     -      |     -     |   31.13  \n","   2    |  1940   |   0.487746   |     -      |     -     |   31.13  \n","   2    |  1960   |   0.489399   |     -      |     -     |   31.14  \n","   2    |  1980   |   0.481207   |     -      |     -     |   31.14  \n","   2    |  2000   |   0.476590   |     -      |     -     |   31.13  \n","F1 macro: 0.7179434794177773\n","   2    |    -    | ------------ |  0.467206  |   75.39   |  3195.22 \n","----------------------------------------------------------------------\n","   2    |  2020   |   0.485811   |     -      |     -     |   50.97  \n","   2    |  2040   |   0.489247   |     -      |     -     |   31.15  \n","   2    |  2060   |   0.475804   |     -      |     -     |   31.14  \n","   2    |  2080   |   0.483245   |     -      |     -     |   31.13  \n","   2    |  2100   |   0.472487   |     -      |     -     |   31.14  \n","   2    |  2120   |   0.477006   |     -      |     -     |   31.14  \n","   2    |  2140   |   0.486146   |     -      |     -     |   31.13  \n","   2    |  2160   |   0.472400   |     -      |     -     |   31.14  \n","   2    |  2180   |   0.469108   |     -      |     -     |   31.13  \n","   2    |  2200   |   0.482008   |     -      |     -     |   31.14  \n","   2    |  2220   |   0.478248   |     -      |     -     |   31.13  \n","   2    |  2240   |   0.482748   |     -      |     -     |   31.14  \n","   2    |  2260   |   0.482803   |     -      |     -     |   31.14  \n","   2    |  2280   |   0.476740   |     -      |     -     |   31.14  \n","   2    |  2300   |   0.484115   |     -      |     -     |   31.13  \n","   2    |  2320   |   0.476188   |     -      |     -     |   31.14  \n","   2    |  2340   |   0.476362   |     -      |     -     |   31.13  \n","   2    |  2360   |   0.491484   |     -      |     -     |   31.14  \n","   2    |  2380   |   0.477621   |     -      |     -     |   31.14  \n","   2    |  2400   |   0.474873   |     -      |     -     |   31.14  \n","   2    |  2420   |   0.473162   |     -      |     -     |   31.14  \n","   2    |  2440   |   0.477241   |     -      |     -     |   31.14  \n","   2    |  2460   |   0.472405   |     -      |     -     |   31.13  \n","   2    |  2480   |   0.475789   |     -      |     -     |   31.13  \n","   2    |  2500   |   0.486235   |     -      |     -     |   31.13  \n","F1 macro: 0.7218499682949044\n","   2    |    -    | ------------ |  0.464993  |   75.78   |  3993.79 \n","----------------------------------------------------------------------\n","   2    |  2520   |   0.468899   |     -      |     -     |   51.29  \n","   2    |  2540   |   0.476556   |     -      |     -     |   31.14  \n","   2    |  2560   |   0.469963   |     -      |     -     |   31.14  \n","   2    |  2580   |   0.484130   |     -      |     -     |   31.14  \n","   2    |  2600   |   0.483578   |     -      |     -     |   31.13  \n","   2    |  2620   |   0.464312   |     -      |     -     |   31.13  \n","   2    |  2640   |   0.486169   |     -      |     -     |   31.14  \n","   2    |  2660   |   0.486516   |     -      |     -     |   31.14  \n","   2    |  2680   |   0.483545   |     -      |     -     |   31.14  \n","   2    |  2700   |   0.482068   |     -      |     -     |   31.13  \n","   2    |  2720   |   0.475120   |     -      |     -     |   31.13  \n","   2    |  2740   |   0.477999   |     -      |     -     |   31.14  \n","   2    |  2760   |   0.478728   |     -      |     -     |   31.14  \n","   2    |  2780   |   0.476543   |     -      |     -     |   31.13  \n","   2    |  2800   |   0.473150   |     -      |     -     |   31.13  \n","   2    |  2820   |   0.481204   |     -      |     -     |   31.13  \n","   2    |  2840   |   0.471383   |     -      |     -     |   31.13  \n","   2    |  2860   |   0.483479   |     -      |     -     |   31.13  \n","   2    |  2880   |   0.478606   |     -      |     -     |   31.14  \n","   2    |  2900   |   0.463292   |     -      |     -     |   31.14  \n","   2    |  2920   |   0.483455   |     -      |     -     |   31.14  \n","   2    |  2940   |   0.481543   |     -      |     -     |   31.14  \n","   2    |  2960   |   0.486380   |     -      |     -     |   31.14  \n","   2    |  2980   |   0.478160   |     -      |     -     |   31.13  \n","   2    |  3000   |   0.478731   |     -      |     -     |   31.14  \n","F1 macro: 0.7213499400752859\n","   2    |    -    | ------------ |  0.464847  |   75.23   |  4792.00 \n","----------------------------------------------------------------------\n","   2    |  3020   |   0.485737   |     -      |     -     |   50.99  \n","   2    |  3040   |   0.479515   |     -      |     -     |   31.14  \n","   2    |  3060   |   0.469191   |     -      |     -     |   31.14  \n","   2    |  3080   |   0.486310   |     -      |     -     |   31.15  \n","   2    |  3100   |   0.490813   |     -      |     -     |   31.14  \n","   2    |  3120   |   0.475690   |     -      |     -     |   31.14  \n","   2    |  3140   |   0.477195   |     -      |     -     |   31.14  \n","   2    |  3160   |   0.479317   |     -      |     -     |   31.14  \n","   2    |  3180   |   0.468735   |     -      |     -     |   31.14  \n","   2    |  3200   |   0.492592   |     -      |     -     |   31.13  \n","   2    |  3220   |   0.481598   |     -      |     -     |   31.14  \n","   2    |  3240   |   0.476187   |     -      |     -     |   31.14  \n","   2    |  3260   |   0.478525   |     -      |     -     |   31.13  \n","   2    |  3280   |   0.477022   |     -      |     -     |   31.14  \n","   2    |  3300   |   0.483009   |     -      |     -     |   31.13  \n","   2    |  3320   |   0.472255   |     -      |     -     |   31.13  \n","   2    |  3340   |   0.477250   |     -      |     -     |   31.13  \n","   2    |  3360   |   0.487500   |     -      |     -     |   31.14  \n","   2    |  3380   |   0.476361   |     -      |     -     |   31.13  \n","   2    |  3400   |   0.484711   |     -      |     -     |   31.12  \n","   2    |  3420   |   0.482243   |     -      |     -     |   31.12  \n","   2    |  3440   |   0.479331   |     -      |     -     |   31.13  \n","   2    |  3460   |   0.485106   |     -      |     -     |   31.13  \n","   2    |  3480   |   0.475705   |     -      |     -     |   31.13  \n","   2    |  3500   |   0.476688   |     -      |     -     |   31.13  \n","F1 macro: 0.7162204671314956\n","   2    |    -    | ------------ |  0.464738  |   75.61   |  5590.22 \n","----------------------------------------------------------------------\n","   2    |  3520   |   0.470539   |     -      |     -     |   50.98  \n","   2    |  3540   |   0.473770   |     -      |     -     |   31.12  \n","   2    |  3560   |   0.470543   |     -      |     -     |   31.12  \n","   2    |  3580   |   0.484991   |     -      |     -     |   31.13  \n","   2    |  3600   |   0.481842   |     -      |     -     |   31.12  \n","   2    |  3620   |   0.473771   |     -      |     -     |   31.13  \n","   2    |  3640   |   0.495370   |     -      |     -     |   31.12  \n","   2    |  3660   |   0.473102   |     -      |     -     |   31.13  \n","   2    |  3680   |   0.480791   |     -      |     -     |   31.12  \n","   2    |  3700   |   0.491082   |     -      |     -     |   31.13  \n","   2    |  3720   |   0.476293   |     -      |     -     |   31.12  \n","   2    |  3740   |   0.477790   |     -      |     -     |   31.13  \n","   2    |  3760   |   0.470984   |     -      |     -     |   31.12  \n","   2    |  3780   |   0.483198   |     -      |     -     |   31.13  \n","   2    |  3800   |   0.488308   |     -      |     -     |   31.13  \n","   2    |  3820   |   0.478114   |     -      |     -     |   31.13  \n","   2    |  3840   |   0.479001   |     -      |     -     |   31.13  \n","   2    |  3860   |   0.483448   |     -      |     -     |   31.14  \n","   2    |  3880   |   0.484803   |     -      |     -     |   31.13  \n","   2    |  3900   |   0.469136   |     -      |     -     |   31.14  \n","   2    |  3920   |   0.469714   |     -      |     -     |   31.13  \n","   2    |  3940   |   0.471987   |     -      |     -     |   31.12  \n","   2    |  3960   |   0.467285   |     -      |     -     |   31.12  \n","   2    |  3980   |   0.477975   |     -      |     -     |   31.12  \n","   2    |  4000   |   0.483586   |     -      |     -     |   31.13  \n","F1 macro: 0.7139907822288685\n","   2    |    -    | ------------ |  0.463886  |   75.43   |  6388.83 \n","----------------------------------------------------------------------\n","   2    |  4020   |   0.466926   |     -      |     -     |   51.52  \n","   2    |  4040   |   0.481112   |     -      |     -     |   31.12  \n","   2    |  4060   |   0.471403   |     -      |     -     |   31.12  \n","   2    |  4080   |   0.478820   |     -      |     -     |   31.13  \n","   2    |  4100   |   0.486669   |     -      |     -     |   31.13  \n","   2    |  4120   |   0.480887   |     -      |     -     |   31.13  \n","   2    |  4140   |   0.480829   |     -      |     -     |   31.13  \n","   2    |  4160   |   0.474373   |     -      |     -     |   31.13  \n","   2    |  4180   |   0.476287   |     -      |     -     |   31.13  \n","   2    |  4200   |   0.483949   |     -      |     -     |   31.16  \n","   2    |  4220   |   0.489452   |     -      |     -     |   31.13  \n","   2    |  4240   |   0.482064   |     -      |     -     |   31.13  \n","   2    |  4260   |   0.482160   |     -      |     -     |   31.14  \n","   2    |  4280   |   0.468127   |     -      |     -     |   31.13  \n","   2    |  4300   |   0.481279   |     -      |     -     |   31.13  \n","   2    |  4320   |   0.477370   |     -      |     -     |   31.12  \n","   2    |  4340   |   0.483225   |     -      |     -     |   31.13  \n","   2    |  4360   |   0.473936   |     -      |     -     |   31.12  \n","   2    |  4380   |   0.471619   |     -      |     -     |   31.12  \n","   2    |  4400   |   0.487858   |     -      |     -     |   31.12  \n","   2    |  4420   |   0.476847   |     -      |     -     |   31.13  \n","   2    |  4440   |   0.479207   |     -      |     -     |   31.12  \n","   2    |  4460   |   0.480231   |     -      |     -     |   31.13  \n","   2    |  4480   |   0.481900   |     -      |     -     |   31.13  \n","   2    |  4500   |   0.473271   |     -      |     -     |   31.13  \n","F1 macro: 0.7192046177395283\n","   2    |    -    | ------------ |  0.463024  |   75.54   |  7186.91 \n","----------------------------------------------------------------------\n","   2    |  4520   |   0.475996   |     -      |     -     |   50.99  \n","   2    |  4540   |   0.479481   |     -      |     -     |   31.13  \n","   2    |  4560   |   0.476312   |     -      |     -     |   31.13  \n","   2    |  4580   |   0.486093   |     -      |     -     |   31.13  \n","   2    |  4600   |   0.481023   |     -      |     -     |   31.14  \n","   2    |  4620   |   0.490249   |     -      |     -     |   31.13  \n","   2    |  4640   |   0.480791   |     -      |     -     |   31.14  \n","   2    |  4660   |   0.480634   |     -      |     -     |   31.13  \n","   2    |  4680   |   0.486051   |     -      |     -     |   31.13  \n","   2    |  4700   |   0.486655   |     -      |     -     |   31.13  \n","   2    |  4720   |   0.482453   |     -      |     -     |   31.13  \n","   2    |  4740   |   0.481428   |     -      |     -     |   31.13  \n","   2    |  4760   |   0.473156   |     -      |     -     |   31.13  \n","   2    |  4780   |   0.484999   |     -      |     -     |   31.13  \n","   2    |  4800   |   0.470200   |     -      |     -     |   31.14  \n","   2    |  4820   |   0.477846   |     -      |     -     |   31.14  \n","   2    |  4840   |   0.485709   |     -      |     -     |   31.14  \n","   2    |  4860   |   0.480692   |     -      |     -     |   31.13  \n","   2    |  4880   |   0.468192   |     -      |     -     |   31.13  \n","   2    |  4900   |   0.497325   |     -      |     -     |   31.13  \n","   2    |  4920   |   0.470518   |     -      |     -     |   31.13  \n","   2    |  4940   |   0.480745   |     -      |     -     |   31.13  \n","   2    |  4960   |   0.482785   |     -      |     -     |   31.13  \n","   2    |  4980   |   0.482886   |     -      |     -     |   31.13  \n","   2    |  5000   |   0.470575   |     -      |     -     |   31.13  \n","F1 macro: 0.718381609334056\n","   2    |    -    | ------------ |  0.462797  |   75.54   |  7985.32 \n","----------------------------------------------------------------------\n","   2    |  5020   |   0.475516   |     -      |     -     |   51.22  \n","   2    |  5040   |   0.471903   |     -      |     -     |   31.13  \n","   2    |  5060   |   0.472798   |     -      |     -     |   31.13  \n","   2    |  5080   |   0.476057   |     -      |     -     |   31.13  \n","   2    |  5100   |   0.470201   |     -      |     -     |   31.13  \n","   2    |  5120   |   0.478858   |     -      |     -     |   31.13  \n","   2    |  5140   |   0.481251   |     -      |     -     |   31.12  \n","   2    |  5160   |   0.473266   |     -      |     -     |   31.13  \n","   2    |  5180   |   0.479010   |     -      |     -     |   31.13  \n","   2    |  5200   |   0.471663   |     -      |     -     |   31.13  \n","   2    |  5220   |   0.474335   |     -      |     -     |   31.13  \n","   2    |  5240   |   0.481853   |     -      |     -     |   31.12  \n","   2    |  5260   |   0.481275   |     -      |     -     |   31.12  \n","   2    |  5280   |   0.474930   |     -      |     -     |   31.12  \n","   2    |  5300   |   0.480337   |     -      |     -     |   31.13  \n","   2    |  5320   |   0.484897   |     -      |     -     |   31.13  \n","   2    |  5340   |   0.468596   |     -      |     -     |   31.13  \n","   2    |  5360   |   0.470472   |     -      |     -     |   31.12  \n","   2    |  5380   |   0.481553   |     -      |     -     |   31.13  \n","   2    |  5400   |   0.474279   |     -      |     -     |   31.13  \n","   2    |  5420   |   0.465719   |     -      |     -     |   31.13  \n","   2    |  5440   |   0.480493   |     -      |     -     |   31.13  \n","   2    |  5460   |   0.480490   |     -      |     -     |   31.13  \n","   2    |  5480   |   0.488366   |     -      |     -     |   31.13  \n","   2    |  5500   |   0.477475   |     -      |     -     |   31.13  \n","F1 macro: 0.7263041975072165\n","   2    |    -    | ------------ |  0.462063  |   75.77   |  8783.35 \n","----------------------------------------------------------------------\n","   2    |  5520   |   0.460283   |     -      |     -     |   51.00  \n","   2    |  5540   |   0.463344   |     -      |     -     |   31.13  \n","   2    |  5560   |   0.475587   |     -      |     -     |   31.14  \n","   2    |  5580   |   0.469531   |     -      |     -     |   31.14  \n","   2    |  5600   |   0.492207   |     -      |     -     |   31.13  \n","   2    |  5620   |   0.489770   |     -      |     -     |   31.13  \n","   2    |  5640   |   0.487797   |     -      |     -     |   31.14  \n","   2    |  5660   |   0.483950   |     -      |     -     |   31.13  \n","   2    |  5680   |   0.466936   |     -      |     -     |   31.13  \n","   2    |  5700   |   0.475958   |     -      |     -     |   31.13  \n","   2    |  5720   |   0.484925   |     -      |     -     |   31.13  \n","   2    |  5740   |   0.472178   |     -      |     -     |   31.13  \n","   2    |  5760   |   0.478967   |     -      |     -     |   31.14  \n","   2    |  5780   |   0.470115   |     -      |     -     |   31.14  \n","   2    |  5800   |   0.471869   |     -      |     -     |   31.14  \n","   2    |  5820   |   0.480773   |     -      |     -     |   31.13  \n","   2    |  5840   |   0.482922   |     -      |     -     |   31.13  \n","   2    |  5860   |   0.485143   |     -      |     -     |   31.13  \n","   2    |  5880   |   0.478114   |     -      |     -     |   31.14  \n","   2    |  5900   |   0.466765   |     -      |     -     |   31.13  \n","   2    |  5920   |   0.473162   |     -      |     -     |   31.14  \n","   2    |  5940   |   0.466588   |     -      |     -     |   31.14  \n","   2    |  5960   |   0.485546   |     -      |     -     |   31.14  \n","   2    |  5980   |   0.472341   |     -      |     -     |   31.13  \n","   2    |  6000   |   0.466849   |     -      |     -     |   31.14  \n","F1 macro: 0.724254417768705\n","   2    |    -    | ------------ |  0.461979  |   75.43   |  9581.60 \n","----------------------------------------------------------------------\n","   2    |  6020   |   0.473470   |     -      |     -     |   50.98  \n","   2    |  6040   |   0.471878   |     -      |     -     |   31.14  \n","   2    |  6060   |   0.471132   |     -      |     -     |   31.14  \n","   2    |  6080   |   0.471144   |     -      |     -     |   31.13  \n","   2    |  6100   |   0.482939   |     -      |     -     |   31.14  \n","   2    |  6120   |   0.476001   |     -      |     -     |   31.13  \n","   2    |  6140   |   0.488267   |     -      |     -     |   31.14  \n","   2    |  6160   |   0.472458   |     -      |     -     |   31.13  \n","   2    |  6180   |   0.464367   |     -      |     -     |   31.14  \n","   2    |  6200   |   0.476357   |     -      |     -     |   31.13  \n","   2    |  6220   |   0.473786   |     -      |     -     |   31.13  \n","   2    |  6240   |   0.479000   |     -      |     -     |   31.13  \n","   2    |  6260   |   0.474664   |     -      |     -     |   31.13  \n","   2    |  6280   |   0.482312   |     -      |     -     |   31.13  \n","   2    |  6300   |   0.479173   |     -      |     -     |   31.14  \n","   2    |  6320   |   0.472368   |     -      |     -     |   31.14  \n","   2    |  6340   |   0.479745   |     -      |     -     |   31.13  \n","   2    |  6360   |   0.481187   |     -      |     -     |   31.13  \n","   2    |  6380   |   0.468615   |     -      |     -     |   31.14  \n","   2    |  6400   |   0.477628   |     -      |     -     |   31.13  \n","   2    |  6420   |   0.465071   |     -      |     -     |   31.14  \n","   2    |  6440   |   0.476212   |     -      |     -     |   31.13  \n","   2    |  6460   |   0.473692   |     -      |     -     |   31.13  \n","   2    |  6480   |   0.475946   |     -      |     -     |   31.14  \n","   2    |  6500   |   0.481550   |     -      |     -     |   31.14  \n","F1 macro: 0.7250375661227004\n","   2    |    -    | ------------ |  0.461536  |   75.75   | 10380.18 \n","----------------------------------------------------------------------\n","   2    |  6520   |   0.476543   |     -      |     -     |   51.32  \n","   2    |  6540   |   0.477819   |     -      |     -     |   31.14  \n","   2    |  6560   |   0.471981   |     -      |     -     |   31.13  \n","   2    |  6580   |   0.472439   |     -      |     -     |   31.13  \n","   2    |  6600   |   0.475445   |     -      |     -     |   31.13  \n","   2    |  6620   |   0.475030   |     -      |     -     |   31.13  \n","   2    |  6640   |   0.460985   |     -      |     -     |   31.13  \n","   2    |  6660   |   0.474634   |     -      |     -     |   31.13  \n","   2    |  6680   |   0.472733   |     -      |     -     |   31.13  \n","   2    |  6700   |   0.470781   |     -      |     -     |   31.13  \n","   2    |  6720   |   0.474241   |     -      |     -     |   31.14  \n","   2    |  6740   |   0.466745   |     -      |     -     |   31.13  \n","   2    |  6760   |   0.470665   |     -      |     -     |   31.13  \n","   2    |  6780   |   0.481203   |     -      |     -     |   31.14  \n","   2    |  6800   |   0.486061   |     -      |     -     |   31.13  \n","   2    |  6820   |   0.474121   |     -      |     -     |   31.13  \n","   2    |  6840   |   0.471402   |     -      |     -     |   31.13  \n","   2    |  6860   |   0.470013   |     -      |     -     |   31.13  \n","   2    |  6880   |   0.480432   |     -      |     -     |   31.13  \n","   2    |  6900   |   0.485294   |     -      |     -     |   31.13  \n","   2    |  6920   |   0.478363   |     -      |     -     |   31.13  \n","   2    |  6940   |   0.476595   |     -      |     -     |   31.13  \n","   2    |  6960   |   0.473639   |     -      |     -     |   31.13  \n","   2    |  6980   |   0.494721   |     -      |     -     |   31.14  \n","   2    |  7000   |   0.486491   |     -      |     -     |   31.13  \n","F1 macro: 0.728004112023958\n","   2    |    -    | ------------ |  0.461489  |   75.73   | 11178.36 \n","----------------------------------------------------------------------\n","   2    |  7020   |   0.481955   |     -      |     -     |   51.06  \n","   2    |  7040   |   0.476368   |     -      |     -     |   31.13  \n","   2    |  7060   |   0.483320   |     -      |     -     |   31.13  \n","   2    |  7080   |   0.478073   |     -      |     -     |   31.13  \n","   2    |  7100   |   0.473057   |     -      |     -     |   31.13  \n","   2    |  7120   |   0.481210   |     -      |     -     |   31.13  \n","   2    |  7140   |   0.480126   |     -      |     -     |   31.13  \n","   2    |  7160   |   0.483596   |     -      |     -     |   31.14  \n","   2    |  7180   |   0.474193   |     -      |     -     |   31.13  \n","   2    |  7200   |   0.477320   |     -      |     -     |   31.15  \n","   2    |  7220   |   0.478814   |     -      |     -     |   31.13  \n","   2    |  7240   |   0.473549   |     -      |     -     |   31.13  \n","   2    |  7260   |   0.470218   |     -      |     -     |   31.13  \n","   2    |  7280   |   0.471313   |     -      |     -     |   31.13  \n","   2    |  7300   |   0.474796   |     -      |     -     |   31.13  \n","   2    |  7320   |   0.477999   |     -      |     -     |   31.13  \n","   2    |  7340   |   0.460873   |     -      |     -     |   31.13  \n","   2    |  7360   |   0.477120   |     -      |     -     |   31.14  \n","   2    |  7380   |   0.469383   |     -      |     -     |   31.13  \n","   2    |  7400   |   0.482808   |     -      |     -     |   31.13  \n","   2    |  7420   |   0.480000   |     -      |     -     |   31.13  \n","   2    |  7440   |   0.479762   |     -      |     -     |   31.13  \n","   2    |  7460   |   0.476244   |     -      |     -     |   31.13  \n","   2    |  7480   |   0.484660   |     -      |     -     |   31.13  \n","   2    |  7500   |   0.475367   |     -      |     -     |   31.13  \n","F1 macro: 0.7286937789808099\n","   2    |    -    | ------------ |  0.461428  |   75.74   | 11976.54 \n","----------------------------------------------------------------------\n","   2    |  7520   |   0.474899   |     -      |     -     |   51.22  \n","   2    |  7540   |   0.470354   |     -      |     -     |   31.14  \n","   2    |  7560   |   0.481794   |     -      |     -     |   31.13  \n","   2    |  7580   |   0.470958   |     -      |     -     |   31.13  \n","   2    |  7600   |   0.473591   |     -      |     -     |   31.13  \n","   2    |  7620   |   0.470919   |     -      |     -     |   31.13  \n","   2    |  7640   |   0.481908   |     -      |     -     |   31.12  \n","   2    |  7660   |   0.469496   |     -      |     -     |   31.14  \n","   2    |  7680   |   0.471664   |     -      |     -     |   31.13  \n","   2    |  7700   |   0.476751   |     -      |     -     |   31.13  \n","   2    |  7720   |   0.474785   |     -      |     -     |   31.13  \n","   2    |  7740   |   0.476614   |     -      |     -     |   31.13  \n","   2    |  7760   |   0.458808   |     -      |     -     |   31.13  \n","   2    |  7780   |   0.469214   |     -      |     -     |   31.12  \n","   2    |  7800   |   0.471940   |     -      |     -     |   31.12  \n","   2    |  7820   |   0.472873   |     -      |     -     |   31.13  \n","   2    |  7840   |   0.475103   |     -      |     -     |   31.13  \n","   2    |  7860   |   0.469137   |     -      |     -     |   31.13  \n","   2    |  7880   |   0.482445   |     -      |     -     |   31.13  \n","   2    |  7900   |   0.475495   |     -      |     -     |   31.13  \n","   2    |  7920   |   0.470996   |     -      |     -     |   31.13  \n","   2    |  7940   |   0.474860   |     -      |     -     |   31.12  \n","   2    |  7960   |   0.469052   |     -      |     -     |   31.13  \n","   2    |  7980   |   0.476990   |     -      |     -     |   31.13  \n","   2    |  8000   |   0.467589   |     -      |     -     |   31.13  \n","F1 macro: 0.7288970182352907\n","   2    |    -    | ------------ |  0.461426  |   75.74   | 12774.89 \n","----------------------------------------------------------------------\n","   2    |  8009   |   0.470795   |     -      |     -     |   34.17  \n","----------------------------------------------------------------------\n","F1 macro: 0.734944432902882\n","   2    |    -    |   0.479243   |  0.468640  |   75.46   | 13910.72 \n","----------------------------------------------------------------------\n","\n","\n","Training complete!\n"]}],"source":["set_seed(42)    # Set seed for reproducibility\n","bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True, accum_steps=1)"]},{"cell_type":"code","source":["torch.save(bert_classifier.state_dict(), \"drive/MyDrive/checkpoint_last_2epoch.pt\")"],"metadata":{"id":"xCkLiwnzSH4g"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T14:26:03.456774Z","iopub.status.busy":"2023-05-22T14:26:03.456341Z","iopub.status.idle":"2023-05-22T14:26:03.741753Z","shell.execute_reply":"2023-05-22T14:26:03.740816Z","shell.execute_reply.started":"2023-05-22T14:26:03.456737Z"},"trusted":true,"id":"OAVSpyL6Vy7-"},"outputs":[],"source":["train_dataloader = MyDataset(train_data).create_dataloader(batch_size=600, shuffle=True, num_workers=1)\n","eval_dataloader = MyDataset(eval_data).create_dataloader(batch_size=2000, num_workers=1)\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","\n","def bert_predict(model, test_dataloader):\n","    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n","    on the test set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    all_logits = []\n","\n","    # For each batch in our test set...\n","    for batch in tqdm(test_dataloader):\n","      b_input_ids, b_attn_mask, b_labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n","\n","        # Compute logits\n","      with torch.no_grad():\n","          logits = model(b_input_ids,b_attn_mask)\n","      all_logits.append(logits)\n","\n","    # Concatenate logits from each batch\n","    all_logits = torch.cat(all_logits, dim=0)\n","\n","    # Apply softmax to calculate probabilities\n","    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n","\n","    return probs\n","\n"]},{"cell_type":"code","source":["device = torch.device(\"cuda\")\n","bert_classifier, o, s = initialize_model(epochs=20)\n","bert_classifier.load_state_dict(torch.load(\"drive/MyDrive/checkpoint_last_20epoch.pt\"))\n","bert_classifier.to(device)\n","probs = bert_predict(bert_classifier, eval_dataloader)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8n3i0qgCX9oy","executionInfo":{"status":"ok","timestamp":1687195445823,"user_tz":-120,"elapsed":4936224,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}},"outputId":"d646b6c3-0a16-4882-e640-88049dd54bb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|| 2953/2953 [1:22:12<00:00,  1.67s/it]\n"]}]},{"cell_type":"code","source":["preds = torch.argmax(torch.Tensor(probs), dim=1).flatten()"],"metadata":{"id":"60PSCO9zmAu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, balanced_accuracy_score, zero_one_loss\n","y_val = eval_data['style_name']\n","accuracy_score(y_val, preds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlpaazRgfch9","executionInfo":{"status":"ok","timestamp":1687195466579,"user_tz":-120,"elapsed":763,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}},"outputId":"2611364b-0cfe-4ffb-8680-1d986fc4c507"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([48,  5, 72,  ..., 89, 72, 19])\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7407493268366002"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["f1_score(y_val, preds.cpu(), average='macro')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXPuIJNDZcDI","executionInfo":{"status":"ok","timestamp":1687195529063,"user_tz":-120,"elapsed":3998,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}},"outputId":"3c7663e7-3c10-4a9f-b16e-4a25956a9730"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.69004770370113"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["precision_score(y_val, preds.cpu(), average='macro')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNLMXU56mk5v","executionInfo":{"status":"ok","timestamp":1687195505006,"user_tz":-120,"elapsed":4291,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}},"outputId":"7bd7a7bd-3c8f-4378-fe0d-a21a23a1647b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7257305957413948"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["recall_score(y_val, preds.cpu(), average='macro')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FaPKRcjrmx7v","executionInfo":{"status":"ok","timestamp":1687195512411,"user_tz":-120,"elapsed":4653,"user":{"displayName":"Anna Smirnova","userId":"01800919117221371573"}},"outputId":"9b0cc097-80e2-461c-9b23-f22ff2b30883"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7067242121336447"]},"metadata":{},"execution_count":22}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}